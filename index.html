<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="NeuralRecon reconstructs 3D scene geometry from a monocular video with known camera poses in real-time. Accepted in CVPR 2021 as oral."/>
    <title>MosaicMVS: Mosaic-based Omnidirectional Multi-view Stereo for Indoor Scenes</title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <style>
      body {
        background: #fdfcf9 no-repeat fixed top left;
        font-family:'Open Sans', sans-serif;
      }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/before-after-slider@1.0.0/dist/slider.bundle.js"></script>
  </head>

  <!-- cover -->
  <section>
    <div class="jumbotron text-center mt-0">
      <div class="container">
        <div class="row">
          <div class="col">
            <h2 style="font-size:30px;">MosaicMVS: Mosaic-based Omnidirectional Multi-view Stereo for Indoor Scenes</h2>
            <!-- <h4 style="color:#6e6e6e;"> CVPR 2021</h4>
            <h5 style="color:#6e6e6e;"> (Oral Presentation and Best Paper Candidate)</h5> -->
            <!-- <hr>
            <h6> <a href="https://jiamingsun.ml/" target="_blank">Jiaming Sun</a><sup>1,2*</sup>, 
                 <a href="https://ymingxie.github.io/" target="_blank">Yiming Xie</a><sup>1*</sup>, 
                 <!-- <a href="https://ymingxie.github.io/" target="_blank">Yiming Xie</a><sup>1,2*</sup>,  -->
                <!-- <a href="https://github.com/f-sky" target="_blank">Linghao Chen</a><sup>1</sup>,
                <a href="http://xzhou.me" target="_blank">Xiaowei Zhou</a><sup>1</sup>,
                <a href="http://www.cad.zju.edu.cn/bao/" target="_blank">Hujun Bao</a><sup>1</sup></h6>
            <p> <sup>1</sup>State Key Lab of CAD & CG, Zhejiang University &nbsp;&nbsp; 
                <sup>2</sup>SenseTime Research
                <br>
                <sup>*</sup> denotes equal contribution -->

            <div class="row justify-content-center">
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="http://vds.sogang.ac.kr/" role="button" target="_blank">
                    <i class="fa fa-file"></i> Paper</a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" id="code_soon" href="https://anonymous.4open.science/r/MosaicMVS-5EE6/" role="button" target="_blank" disabled="disabled">
                <i class="fa fa-github-alt"></i> Code </a> </p>
              </div>
              <div class="column">
                  <p class="mb-5"><a class="btn btn-large btn-light" href="files/supp.pdf" role="button"  target="_blank">
                    <i class="fa fa-file"></i> Supplementary</a> </p>
              </div>
            </div>
            
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- abstract -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
          <h3>Abstract</h3>
            <!-- <h6 style="color:#8899a5"> NeuralRecon reconstructs 3D scene geometry from a monocular video with known camera poses in <b style="color:#e94a00">real-time</b>üî•.</h6> -->
<!--             <video poster="images/header-vid-poster.png" width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid"> -->
            <!-- <video width="70%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid">
                  <source src="videos/web-scene2.m4v" type="video/mp4">
            </video> -->
            <!--div><b style="color:#fd5638; font-size:large" id="demo-warning"></b>
            <br>
            </div-->
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/Figure_1_4.png" alt="overview">
            <hr style="margin-top:0px">
          <p class="text-justify">
              We present MosaicMVS, a novel learning-based depth estimation framework for a mosaic-based omnidirectional multi-view stereo (MVS) camera setup. 
              It uses a regular field of view (FOV) MVS network for an omnidirectional imaging setup with explicit consideration of hypothetical voxel-wise FOV overlaps. 
              The resulting depth predictions are accurate and agree on the omnidirectional multi-view geometry. 
              Unlike existing MVS setups, MosaicMVS camera setup can be easily applied to omnidirectional indoor scenes without having to account for constraints such as intricate epipolar constraints and the distortion of omnidirectional cameras. 
              We validate the effectiveness of our framework on a new challenging indoor dataset in terms of depth estimation, reconstruction, and view synthesis. Our framework outperforms the state-of-the-art methods in a large margin in all test scenes.
          </p>
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- Depth estimation showcases -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Depth estimation showcases</h3>
            <hr style="margin-top:0px">
            <div id="Slider3"></div>
              <script>
              new SliderBar({
                el: '#Slider3',            // The container, required
                beforeImg: 'depths/159_rgb.png',  // before image, required
                afterImg: 'depths/159_ours.png',    // after image, required
                width: "100%",               // slide-wrap width, default 100%
                height: "400px",            // slide-wrap height, default image-height
                line: true,                 // Dividing line, default true
                lineColor: "rgba(0,0,0,0.5)" // Dividing line color, default rgba(0,0,0,0.5)
              });
              </script>
            <!--hr style="margin-top:0px"-->
            <hr style="margin-top:10px">
            <div id="Slider4"></div>
            <script>
            new SliderBar({
              el: '#Slider4',            // The container, required
              beforeImg: 'depths/238_rgb.png',  // before image, required
              afterImg: 'depths/238_ours.png',    // after image, required
              width: "100%",               // slide-wrap width, default 100%
              height: "400px",            // slide-wrap height, default image-height
              line: true,                 // Dividing line, default true
              lineColor: "rgba(0,0,0,0.5)" // Dividing line color, default rgba(0,0,0,0.5)
            });
            </script>
          <hr style="margin-top:10px">
            <p class="text-justify">
              From posed RGBs with divergent camera setup (left), our framework estimates accurate depth maps (right).
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- Depth map comparison -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Depth map comparison</h3>
            <hr style="margin-top:0px">
            <div id="Slider1"></div>
              <script>
              new SliderBar({
                el: '#Slider1',            // The container, required
                beforeImg: 'depths/159_comp.png',  // before image, required
                afterImg: 'depths/159_ours.png',    // after image, required
                width: "100%",               // slide-wrap width, default 100%
                height: "400px",            // slide-wrap height, default image-height
                line: true,                 // Dividing line, default true
                lineColor: "rgba(0,0,0,0.5)" // Dividing line color, default rgba(0,0,0,0.5)
              });
              </script>
            <!--hr style="margin-top:0px"-->
            <hr style="margin-top:10px">
            <div id="Slider2"></div>
            <script>
            new SliderBar({
              el: '#Slider2',            // The container, required
              beforeImg: 'depths/238_comp.png',  // before image, required
              afterImg: 'depths/238_ours.png',    // after image, required
              width: "100%",               // slide-wrap width, default 100%
              height: "400px",            // slide-wrap height, default image-height
              line: true,                 // Dividing line, default true
              lineColor: "rgba(0,0,0,0.5)" // Dividing line color, default rgba(0,0,0,0.5)
            });
            </script>
          <hr style="margin-top:10px">
            <p class="text-justify">
              Comparison of estimated depth maps. 
              Comparing the baseline result (left) and ours (right), depth maps produced by our pipeline clearly show fewer artifacts.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- reconstruction showcases -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Reconstruction showcases</h3>
            <hr style="margin-top:0px">
            <div class="embed-responsive embed-responsive-16by9">
                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://sketchfab.com/playlists/embed?collection=2e16bec6729b4c53b92ed8b674f70338&autostart=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
            </div>
            <br>
            <p class="text-justify">
              Poisson reconstructions with the depth maps produced via COLMAP.
              </p>
        </div>
      </div>
    </div>
  </section>
  <section>
    <div class="container">
      <div class="row">
        <!-- <div class="col-12 text-center">
            <h3>Reconstruction showcases</h3>
            <hr style="margin-top:0px"> -->
            <div class="embed-responsive embed-responsive-16by9">
                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://sketchfab.com/playlists/embed?collection=5a49cab0570e4d9fb610978a640fe080&autostart=1" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
            </div>
            <br>
            <p class="text-justify"> 
              Poisson reconstructions with the depth maps produced via our pipeline. 
              Zoom in by scrolling. 
              You can toggle the ‚ÄúSingle Sided‚Äù option in Model Inspector (pressing I key) to enable back-face culling (see through walls). 
              Select ‚ÄúMatcap‚Äù to inspect the geometry without textures.</p>
        </div>
      </div>
    </div>
  </section>
  <br>



  <!-- real-time incremental reconstruction -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Real-time incremental reconstruction</h3>
            <p class="text-justify"> 
              Data is captured around the working area with an iPhone, and the camera poses are obtained from <a href="https://developer.apple.com/documentation/arkit">ARKit</a>.
              The model used here is only trained on ScanNet, which indicates that NeuralRecon generalizes well to new domains.
              The gradual refinement on the reconstruction quality over time (through GRU-Fusion) can also be observed.
            </p>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="">
                <source src="videos/web-scene1.m4v" type="video/mp4">
            </video>
            <div class="embed-responsive embed-responsive-16by9">
                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://sketchfab.com/playlists/embed?autostart=1&autospin=0.25&amp;collection=3d55b6141e18492790509fc93fa453c9" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
            </div>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col text-center">
            <h3>AR demo 1</h3>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/ar-1.mp4" type="video/mp4">
            </video>
        </div>
        <div class="col text-center">
            <h3>AR demo 2</h3>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" controls autoplay loop="loop" preload="" muted="">
                <source src="videos/ar-2.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Generalization to the outdoor scene</h3>
            <p class="text-justify"> 
              The pretrained model of NeuralRecon can generalize reasonably well to outdoor scenes, which are completely out of the domain of the training dataset ScanNet.
            </p>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" controls preload="" muted="">
                <source src="videos/outdoor-midres-8.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br>

  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Handling scenes with extremely low texture</h3>
            <p class="text-justify"> 
              NeuralRecon can handle homogeneous textures (e.g. white walls and tables), thanks to the learned surface priors.
            </p>
            <hr style="margin-top:0px">
            <video width="100%" playsinline="" controls  loop="loop" preload="" muted="">
                <source src="videos/textureless-midres-8.mp4" type="video/mp4">
            </video>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- overview video -->
  <!--section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Overview video (5 min)</h3>
            <hr style="margin-top:0px">
            <div class="embed-responsive embed-responsive-16by9">
                <iframe style="clip-path: inset(1px 1px)" width="100%" height="100%" src="https://www.youtube.com/embed/wuMPaUTJuO0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
            </div>
        </div>
      </div>
    </div>
  </section>
  <br>

  <-- Pipeline overview -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Pipeline overview</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/Figure_2_6.png" alt="MosaicMVS Architechture">
            <hr style="margin-top:0px">
            <p class="text-justify">
              Overview of the proposed MosaicMVS. 
              For every target-source pair, the estimated camera pose and depth values, and source view indices are pre-processed and they were used to infer depth maps. 
              The estimated depth and confidence map were obtained by constructing valid view cost volume regularization in 3D convolution and they were filtered using the pre-processed data without depth values.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>

  <!-- valid view cost volume overview -->
  <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Valid view cost volume overview</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/Figure_6_6.png" alt="valid view cost volume">
            <hr style="margin-top:0px">
            <p class="text-justify">
              The construction process of the valid view cost volume ${{C}}_{V}$. 
              From ${O}_{sum}$, we adaptively change the denominator ${O}_{filled}$ of the cost metric per hypothetical voxel based on each warped feature volume $\tilde{{F}}_i(v)$. 
              Also, we minimize the effect of zero overlapping hypothetical voxels in ${O}_{filled}$.
            </p>
        </div>
      </div>
    </div>
  </section>
  <br>


  <!-- Comparison with state-of-the-art methods -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with state-of-the-art methods</h3>
            <p class="text-left"> 
             Only the inference time on key frames is computed. Back-face culling is enabled during rendering. Ground-truth is captured using the LiDAR sensor on iPad Pro.</p>
            <hr style="margin-top:0px">
            <p class="text-left" style="color:#646464"> B5-Scene 1:</p>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
                <source src="videos/Comparison1-near.m4v" type="video/mp4">
            </video>
            <p class="text-left" style="color:#646464"> B5-Scene 2:</p>
            <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls>
              <source src="videos/Comparison2-near.m4v" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </section>
  <br>

    <!-- Comparison with Atlas -->
    <!-- <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
              <h3>Comparison with <a href="http://zak.murez.com/atlas">Atlas</a> on a large scene (30m x 10m)</h3>
              <hr style="margin-top:0px">
              <video width="100%" playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" controls="">
                <source src="videos/Comparison-atlas.m4v" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </section> -->
    <br>   

  <!-- Comparison with depth-based methods -->
  <!-- <section>
    <div class="container">
      <div class="row">
        <div class="col-12 text-center">
            <h3>Comparison with depth-based methods</h3>
            <hr style="margin-top:0px">
            <img class="img-fluid" src="images/compare-depth-based.png" alt="Comparison with depth-based methods">
        </div>
      </div>
    </div>
  </section>
  <br> -->

  <!-- ack -->
  <!-- <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Acknowledgement</h3>
          <hr style="margin-top:0px">
          <p>
            We would like to specially thank to Reviewer 3 for the positive and constructive comments.
          </p>
          <hr>
      </div>
    </div>
  </div> -->

  <!-- citing -->
  <div class="container">
    <div class="row ">
      <div class="col-12">
          <h3>Citation</h3>
          <hr style="margin-top:0px">
              <pre style="background-color: #e9eeef;padding: 1.25em 1.5em">
<code>@article{,
  title={{MosaicMVS}: : Mosaic-based Omnidirectional Multi-view Stereo for Indoor Scenes},
  author={},
  journal={},
  year={2022}
}</code></pre>
          <hr>
      </div>
    </div>
  </div>

  <footer class="text-center" style="margin-bottom:10px; font-size: medium;">
      <hr>
      Thanks to <a href="https://lioryariv.github.io/" target="_blank">Lior Yariv</a> for the <a href="https://lioryariv.github.io/idr/" target="_blank">website template</a>.
  </footer>

  <script type="text/javascript">
    function changePlaybackSpeed(speed)
        {
            document.getElementById('inspect_vid').playbackRate = speed;
        }
        // changePlaybackSpeed(0.25)

    var demo = document.getElementById("header_vid");
    var startTime;
    var timeout = undefined;
    demo.addEventListener("loadstart", function() {
      startTime = Date.now();
      timeout = setTimeout(function () {
        var demoWarning = document.getElementById("demo-warning");
        var giteeLink = document.createElement("a");
        giteeLink.innerText = "mirror hosted in mainland China";
        giteeLink.href = "https://project-pages-1255496016.cos-website.ap-shanghai.myqcloud.com/neuralrecon/";
        // var bilibiliLink = document.createElement("a");
        // var youtubeLink = document.createElement("a");
        // bilibiliLink.innerText = "BiliBili";
        // bilibiliLink.href = "";
        // youtubeLink.innerText = "YouTube";
        // youtubeLink.href = "";

        demoWarning.append("Loading the videos took too long, you can optionally visit this site in the ", giteeLink, ".");
        // demoWarning.append("Loading the video took too long, you can optionally watch it on Bilibili", bilibiliLink, " or YouTube", youtubeLink, ".");
        clearTimeout(timeout);
        timeout = undefined;
      }, 6000);
    });
    demo.addEventListener("loadeddata", function() {
      if (timeout) {
        clearTimeout(timeout);
        timeout = undefined;
      }
    });
//     var source = document.createElement("source");
//     source.setAttribute("src", "/videos/web-scene2.m4v");
//     source.setAttribute("type", "video/webm");
//     demo.appendChild(source);
  </script>
  <script>
    MathJax = {
      tex: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>  
</body>
</html>
